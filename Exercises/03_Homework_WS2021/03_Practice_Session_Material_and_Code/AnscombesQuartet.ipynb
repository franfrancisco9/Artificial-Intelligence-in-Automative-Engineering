{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import useful stuff\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import subplot, scatter, plot, axis, grid, figure, subplots\n",
    "from sklearn import preprocessing as sk_preprocessing\n",
    "from sklearn import linear_model as sk_linear_model\n",
    "from sklearn import metrics as sk_metrics\n",
    "\n",
    "# Anscombes quartet can be found on Wikepedia \n",
    "# https://en.wikipedia.org/wiki/Anscombe%27s_quartet\n",
    "# Definition of the dataset:\n",
    "\n",
    "x1 = np.array([10.0, 8.0,  13.0,  9.0,  11.0, 14.0, 6.0,  4.0,  12.0,  7.0,  5.0])\n",
    "y1 = np.array([8.04, 6.95, 7.58,  8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68])\n",
    "x2 = np.array([10.0, 8.0,  13.0,  9.0,  11.0, 14.0, 6.0,  4.0,  12.0,  7.0,  5.0])\n",
    "y2 = np.array([9.14, 8.14, 8.74,  8.77, 9.26, 8.10, 6.13, 3.10, 9.13,  7.26, 4.74])\n",
    "x3 = np.array([10.0, 8.0,  13.0,  9.0,  11.0, 14.0, 6.0,  4.0,  12.0,  7.0,  5.0])\n",
    "y3 = np.array([7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15,  6.42, 5.73])\n",
    "x4 = np.array([8.0,  8.0,  8.0,   8.0,  8.0,  8.0,  8.0,  19.0,  8.0,  8.0,  8.0])\n",
    "y4 = np.array([6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.50, 5.56, 7.91, 6.89])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic visual analysis of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize dataset to gain some insight on whats going on \n",
    "f, axs = subplots(2,2,dpi=600)\n",
    "ax1 = subplot(2, 2, 1)\n",
    "scatter(x1, y1)\n",
    "grid()\n",
    "subplot(2, 2, 2, sharex=ax1, sharey=ax1)\n",
    "scatter(x2, y2)\n",
    "grid()\n",
    "subplot(2, 2, 3, sharex=ax1, sharey=ax1)\n",
    "scatter(x3, y3)\n",
    "grid()\n",
    "subplot(2, 2, 4, sharex=ax1, sharey=ax1)\n",
    "scatter(x4, y4)\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The intersting thing about Anscombes Quartet is\n",
    "# that the datasets look very different from a graphical point of view \n",
    "# but are nearly similar from a basic statistics point of view. \n",
    "meanValues = np.array([np.mean(y1), np.mean(y2), np.mean(y3), np.mean(y4)])\n",
    "print(\"The mean values are {0:5.3f}, {1:5.3f}, {2:5.3f} and {3:5.3f}\".format(meanValues[0], meanValues[1], meanValues[2], meanValues[3]))\n",
    "stdValues = np.array([np.std(y1), np.std(y2), np.std(y3), np.std(y4)])\n",
    "print(\"The standard deviations are {0:5.3f}, {1:5.3f}, {2:5.3f} and {3:5.3f}\".format(stdValues[0], stdValues[1], stdValues[2], stdValues[3]))\n",
    "corrValues = np.array([np.corrcoef(x1, y1)[0,1], np.corrcoef(x2, y2)[0,1], np.corrcoef(x3, y3)[0,1], np.corrcoef(x4, y4)[0,1]])\n",
    "print(\"The correlation coefficients are {0:5.3f}, {1:5.3f}, {2:5.3f} and {3:5.3f}\".format(corrValues[0], corrValues[1], corrValues[2], corrValues[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Short summary of what we've seen so far \n",
    "We have seen that the collection of four datasets called Anscombes Quartet looks very different from a visual point of view. However, applying basic statistic measures leads to the conclusion that the datasets are indeed very similar. This was exactly the intention for the formulation of this dataset. It was designed to emphasize the importance of visualization during statistical work with real world data. Furthermore, we can see that outliers can significantly reduce performance of basic statistic methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression on that dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create polynomial features for all datasets using linear and quadratic functions \n",
    "polydeg1 = sk_preprocessing.PolynomialFeatures(1)\n",
    "polydeg2 = sk_preprocessing.PolynomialFeatures(2)\n",
    "\n",
    "# create design matrices for all datasets\n",
    "# reshape is necessary to create 2D array from 1D array \n",
    "ds1_pd1_x = polydeg1.fit_transform(x1.reshape(-1,1))\n",
    "ds2_pd1_x = polydeg1.fit_transform(x2.reshape(-1,1))\n",
    "ds3_pd1_x = polydeg1.fit_transform(x3.reshape(-1,1))\n",
    "ds4_pd1_x = polydeg1.fit_transform(x4.reshape(-1,1))\n",
    "ds1_pd2_x = polydeg2.fit_transform(x1.reshape(-1,1))\n",
    "ds2_pd2_x = polydeg2.fit_transform(x2.reshape(-1,1))\n",
    "ds3_pd2_x = polydeg2.fit_transform(x3.reshape(-1,1))\n",
    "ds4_pd2_x = polydeg2.fit_transform(x4.reshape(-1,1))\n",
    "\n",
    "# create regression model objects \n",
    "ds1_pd1_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "ds2_pd1_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "ds3_pd1_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "ds4_pd1_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "ds1_pd2_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "ds2_pd2_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "ds3_pd2_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "ds4_pd2_reg = sk_linear_model.LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Train the models based on the training design matrix and the training output data\n",
    "ds1_pd1_reg.fit(ds1_pd1_x, y1)\n",
    "ds2_pd1_reg.fit(ds2_pd1_x, y2)\n",
    "ds3_pd1_reg.fit(ds3_pd1_x, y3)\n",
    "ds4_pd1_reg.fit(ds4_pd1_x, y4)\n",
    "ds1_pd2_reg.fit(ds1_pd2_x, y1)\n",
    "ds2_pd2_reg.fit(ds2_pd2_x, y2)\n",
    "ds3_pd2_reg.fit(ds3_pd2_x, y3)\n",
    "ds4_pd2_reg.fit(ds4_pd2_x, y4)\n",
    "\n",
    "# Predict the output data for the evaluation points \n",
    "x_eval = np.linspace(0, 20, num=20)\n",
    "ds1_pd1_x_eval = polydeg1.fit_transform(x_eval.reshape(-1,1))\n",
    "ds2_pd1_x_eval = polydeg1.fit_transform(x_eval.reshape(-1,1))\n",
    "ds3_pd1_x_eval = polydeg1.fit_transform(x_eval.reshape(-1,1))\n",
    "ds4_pd1_x_eval = polydeg1.fit_transform(x_eval.reshape(-1,1))\n",
    "ds1_pd2_x_eval= polydeg2.fit_transform(x_eval.reshape(-1,1))\n",
    "ds2_pd2_x_eval = polydeg2.fit_transform(x_eval.reshape(-1,1))\n",
    "ds3_pd2_x_eval = polydeg2.fit_transform(x_eval.reshape(-1,1))\n",
    "ds4_pd2_x_eval = polydeg2.fit_transform(x_eval.reshape(-1,1))\n",
    "\n",
    "ds1_pd1_y_eval = ds1_pd1_reg.predict(ds1_pd1_x_eval)\n",
    "ds2_pd1_y_eval = ds2_pd1_reg.predict(ds2_pd1_x_eval)\n",
    "ds3_pd1_y_eval = ds3_pd1_reg.predict(ds3_pd1_x_eval)\n",
    "ds4_pd1_y_eval = ds4_pd1_reg.predict(ds4_pd1_x_eval)\n",
    "ds1_pd2_y_eval = ds1_pd2_reg.predict(ds1_pd2_x_eval)\n",
    "ds2_pd2_y_eval = ds2_pd2_reg.predict(ds2_pd2_x_eval)\n",
    "ds3_pd2_y_eval = ds3_pd2_reg.predict(ds3_pd2_x_eval)\n",
    "ds4_pd2_y_eval = ds4_pd2_reg.predict(ds4_pd2_x_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize linear fits \n",
    "f, axs = subplots(2,2,dpi=600)\n",
    "ax1 = subplot(2, 2, 1)\n",
    "scatter(x1, y1)\n",
    "grid()\n",
    "plot(x_eval, ds1_pd1_y_eval,'r')\n",
    "subplot(2, 2, 2, sharex=ax1, sharey=ax1)\n",
    "scatter(x2, y2)\n",
    "grid()\n",
    "plot(x_eval, ds2_pd1_y_eval,'r')\n",
    "subplot(2, 2, 3, sharex=ax1, sharey=ax1)\n",
    "scatter(x3, y3)\n",
    "grid()\n",
    "plot(x_eval, ds3_pd1_y_eval,'r')\n",
    "subplot(2, 2, 4, sharex=ax1, sharey=ax1)\n",
    "scatter(x4, y4)\n",
    "grid()\n",
    "plot(x_eval, ds4_pd1_y_eval,'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on the results\n",
    "\n",
    "We have pointed out before, that the basic statistics of all four datasets are nearly equal. Why this might not be a problem in some applications, applying a purely linear model to the dataset shows that this can lead to severe problems. From a visual perspective, everyone would agree that some of the models do not represent the data at all. However, they are optimal in a sense that they minimize the loss function. The quartet therefore emphasizes the importance of data visualization especially if the amount and type of outliers and general behavior of the data is unknown! Furthermore, these datasets can be viewed as warning examples when applying regression methods online in fully autonomos machines. The algorithms have to be capable of dealing with such situations if they may occur. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize quadratic fits \n",
    "f, axs = subplots(2,2,dpi=600)\n",
    "ax1 = subplot(2, 2, 1)\n",
    "scatter(x1, y1)\n",
    "grid()\n",
    "plot(x_eval, ds1_pd2_y_eval,'r')\n",
    "subplot(2, 2, 2, sharex=ax1, sharey=ax1)\n",
    "scatter(x2, y2)\n",
    "grid()\n",
    "plot(x_eval, ds2_pd2_y_eval,'r')\n",
    "subplot(2, 2, 3, sharex=ax1, sharey=ax1)\n",
    "scatter(x3, y3)\n",
    "grid()\n",
    "plot(x_eval, ds3_pd2_y_eval,'r')\n",
    "subplot(2, 2, 4, sharex=ax1, sharey=ax1)\n",
    "scatter(x4, y4)\n",
    "grid()\n",
    "plot(x_eval, ds4_pd2_y_eval,'r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
