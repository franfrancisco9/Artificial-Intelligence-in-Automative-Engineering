{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc9ca03b",
   "metadata": {},
   "source": [
    "# Homework 11 - Reinforcement Learning\n",
    "\n",
    "In this homework, we consider the grid world shown in the lecture and the random policy, that assigns equal probability 1/4 to each action.\n",
    "![title](img/environment.png) \n",
    "![title](img/policy.png)\n",
    "\n",
    "Implement a slightly changed version of the iterative policy evaluation algorithm which can be seen below. The changes are \n",
    "\n",
    "1. the algorithm will not run to convergence, but for a certain number of episodes $N$ \n",
    "2. the value function is updated immediately for each state, instead of saving an old $V_k$ and new $V_{k+1}$\n",
    "\n",
    "The update is thus $V_{new}(x_k) = \\sum_i \\pi(u_i|x_k) (r_{t+1} + V(x_{t+1})|x_t=x_i)$, $V(x_k) = V_{new}(x_k)$. Initialize V as a numpy array of zeros. Iterate starting at state $x=0$ to state $x=10$. We consider the undiscount case ($\\gamma = 1$).\n",
    "\n",
    "![title](img/iteration.png)\n",
    "\n",
    "The function to implement is called 'pol_eval' and takes a single input $N$ describing the number of iterations over all states. The function returns a numpy array $V$, which contains $V(x_i)$ in the $V[i]$.\n",
    "\n",
    "The environment is available and can be called as $x2, r = \\text{gridworld}(x, u)$, the same way as in the code example after the lecture. Use this notebook to implement and test your code and input the desired solution in Moodle.\n",
    "\n",
    "**Function Name:** pol_eval\n",
    "\n",
    "**Function Input Name(s):** N\n",
    "\n",
    "**Function Input Type(s):** integer > 0\n",
    "\n",
    "**Output Type:** numpy array with 12 entries of type float"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c181720e",
   "metadata": {},
   "source": [
    "## Environment\n",
    "This function defines the grid world environment presented in the lecture. The actions are defined as follows:  \n",
    "u = 0: go right  \n",
    "u = 1: go up  \n",
    "u = 2: go left  \n",
    "u = 3: go down  \n",
    "The input is the current state $x$ and the chosen action $u$, the output is the next state $x2$ and the reward $r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8233b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gridworld(x, u):\n",
    "    # Transition Matrix\n",
    "    T = np.array([\n",
    "        [0, 1, 0, 0],\n",
    "        [1, 2, 1, 0],\n",
    "        [3, 2, 2, 1],\n",
    "        [8, 4, 2, 3],\n",
    "        [5, 4, 4, 3],\n",
    "        [6, 5, 4, 8],\n",
    "        [6, 6, 5, 7],\n",
    "        [7, 6, 8, 10],\n",
    "        [7, 5, 3, 9],\n",
    "        [10, 8, 9, 9],\n",
    "        [10, 7, 9, 11],\n",
    "        [11, 11, 11, 11]\n",
    "    ], dtype = int)\n",
    "    \n",
    "    # Get the next state\n",
    "    x2 = T[x, u]\n",
    "    \n",
    "    # Reward\n",
    "    r = -1\n",
    "    if x2 in [8, 9]:\n",
    "        r = -2\n",
    "    elif x == 11:\n",
    "        r = 0\n",
    "    \n",
    "    # Return the next state and reward\n",
    "    return x2, r\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a2a14",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "Write your function in here, the return value should be a numpy array of length 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7ca0fedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pol_eval(N):\n",
    "    # The update is thus $V_{new}(x_k) = \\sum_i \\pi(u_i|x_k) (r_{t+1} + V(x_{t+1})|x_t=x_i)$, $V(x_k) = V_{new}(x_k)$. Initialize V as a numpy array of zeros. Iterate starting at state $x=0$ to state $x=10$. We consider the undiscount case ($\\gamma = 1$).\n",
    "    #\n",
    "    # Input:\n",
    "    # N: number of iterations\n",
    "    #\n",
    "    # Output:\n",
    "    # V: the value function\n",
    "    \n",
    "    # Initialize the value function\n",
    "    V = np.zeros(12)\n",
    "    \n",
    "    # Iterate\n",
    "    for i in range(N):\n",
    "        # Iterate over all states\n",
    "        for x in range(12):\n",
    "            v_new = 0\n",
    "            # Iterate over all actions\n",
    "            for u in range(4):\n",
    "                \n",
    "                # Get the next state and reward\n",
    "                x2, r = gridworld(x, u)\n",
    "                \n",
    "                # Update the value function\n",
    "                v_new += 0.25 * (r + V[x2])\n",
    "\n",
    "            # Update the value function\n",
    "            V[x] = v_new\n",
    "\n",
    "    return V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8752acaa",
   "metadata": {},
   "source": [
    "The implemented function will be called for different values of iterations. A run over 2000 iterations should produce the converged result V = [-126.5 -122.5 -114.5 -102.5 -100 -93.5 -87.5 -77.5 -88 -73.5 -52 0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "401a6199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value function for 10 iterations: \n",
      " [-11.63623989 -12.82782465 -14.16351161 -15.8744503  -15.85803041\n",
      " -16.47118029 -15.8828739  -15.49397572 -17.02839857 -16.44733296\n",
      " -12.0385665    0.        ] \n",
      "\n",
      "The value of state 0 is -11.636\n",
      "The value function for 50 iterations: \n",
      " [-61.31923774 -61.31007531 -60.06815823 -57.26136405 -56.87346639\n",
      " -54.49602868 -51.5672595  -46.30104106 -51.83545685 -44.74655729\n",
      " -31.90789587   0.        ] \n",
      "\n",
      "The value function for 100 iterations: \n",
      " [-96.64978385 -94.50860815 -89.64606883 -81.89602998 -80.38349778\n",
      " -75.77267128 -71.17845074 -63.32549461 -71.55546839 -60.43453379\n",
      " -42.87183495   0.        ] \n",
      "\n",
      "The value of state 0 is -96.65\n",
      "The value function for 2000 iterations: \n",
      " [-126.5 -122.5 -114.5 -102.5 -100.   -93.5  -87.5  -77.5  -88.   -73.5\n",
      "  -52.     0. ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "iterations = [10, 50, 100, 2000]\n",
    "\n",
    "for N in iterations:\n",
    "    V = pol_eval(N)\n",
    "    print(f\"The value function for {N} iterations: \\n {V} \\n\")\n",
    "    if N == 2000:\n",
    "        # check if V = [-126.5, -122.5, -114.5, -102.5, -100, -93.5, -87.5, -77.5, -88, -73.5, -52,0]\n",
    "        assert np.allclose(V, [-126.5, -122.5, -114.5, -102.5, -100, -93.5, -87.5, -77.5, -88, -73.5, -52,0]), \"The value function is not correct\"\n",
    "    \n",
    "    if N == 10:\n",
    "        # print state 0 rounded to 3 decimals\n",
    "        print(f\"The value of state 0 is {round(V[0], 3)}\")\n",
    "    if N == 100:\n",
    "        # print state 0 rounded to 3 decimals\n",
    "        print(f\"The value of state 0 is {round(V[0], 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc33e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "9aff9e50adfaa9e30c910fb3872ffdc72747acb5f50803ca0504f00e980f7c25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
